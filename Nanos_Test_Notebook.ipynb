{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Nanos_Test_Notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1rhyGrIvVEIHc0I1gCzMN2GZaBQ5qYOsi",
      "authorship_tag": "ABX9TyPO/2Wn8h5/zH564CYYknbR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prathamsss/Relevant_Words_Analysis_NLP/blob/main/Nanos_Test_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8SH6M5GCh4p"
      },
      "source": [
        "import bs4 as bs\n",
        "import urllib.request\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lrz77pYbhWZE",
        "outputId": "5c35a2ab-4dd6-44ad-c249-a44aa96140d2"
      },
      "source": [
        "!python -m spacy download en"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (51.1.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcoOdHRGCtbN",
        "outputId": "b543aef4-13db-4500-c296-93dfde398c5c"
      },
      "source": [
        "! echo \"Installing Magnitude.... (please wait, can take a while)\"\n",
        "! (curl https://raw.githubusercontent.com/plasticityai/magnitude/master/install-colab.sh | /bin/bash 1>/dev/null 2>/dev/null)\n",
        "! echo \"Done installing Magnitude.\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Installing Magnitude.... (please wait, can take a while)\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   137  100   137    0     0    352      0 --:--:-- --:--:-- --:--:--   351\n",
            "Done installing Magnitude.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTINj2-ehnWm",
        "outputId": "26651211-3931-4f11-894b-ee2b81055973"
      },
      "source": [
        "import bs4 as bs\n",
        "import urllib.request\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from pymagnitude import MagnitudeUtils\n",
        "from pymagnitude import Magnitude\n",
        "from pymagnitude import Magnitude\n",
        "import numpy as np\n",
        "from itertools import product\n",
        "import en_core_web_sm\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import os\n",
        "class Similarity(object):\n",
        "    \n",
        "  def __init__(self,url,model_path):\n",
        "    \n",
        "    global article_text,nlp\n",
        "    # try:\n",
        "    scrapped_data = urllib.request.urlopen(url)#'https://github.com/plasticityai/magnitude'\n",
        "    article = scrapped_data .read()\n",
        "\n",
        "    parsed_article = bs.BeautifulSoup(article,'lxml')\n",
        "    \n",
        "   \n",
        "\n",
        "    paragraphs = parsed_article.find_all('p')\n",
        "    article_text = \"\"\n",
        "    for p in paragraphs:\n",
        "        article_text += p.text\n",
        "        \n",
        "    nlp =  en_core_web_sm.load()\n",
        "       \n",
        "        \n",
        "  def process_text(self,text):\n",
        "         \n",
        "          text = text.lower()\n",
        "          text=text.replace('{html}',\"\") \n",
        "          text = text.replace(\"\\n\",\"\")\n",
        "          cleanr = re.compile('<.*?>')\n",
        "          text = re.sub(cleanr, '', text)\n",
        "          text = text.strip()\n",
        "          rem_url=re.sub(r'http\\S+', '',text)\n",
        "          text = re.sub('[0-9]+', '', rem_url)\n",
        "          \n",
        "          return [token.text for token in nlp(text) if not token.is_stop]\n",
        "          \n",
        "\n",
        "\n",
        "  def init_model(self):\n",
        "                \n",
        "                 model = Magnitude(model_path)\n",
        "                 return model    \n",
        "     \n",
        "        \n",
        "  def text2vec(self,doc_tok, model, dim=300):\n",
        "      doc_embedding = np.zeros(dim)\n",
        "      valid_words = 0\n",
        "      # print((doc_tok))\n",
        "      for word in doc_tok:\n",
        "         \n",
        "          if word in model:\n",
        "             \n",
        "              valid_words += 1\n",
        "              doc_embedding += model.query(word)\n",
        "          else:\n",
        "              continue\n",
        "      if valid_words > 0:\n",
        "          return doc_embedding / valid_words\n",
        "      else:\n",
        "          return doc_embedding\n",
        " \n",
        "    \n",
        "      \n",
        "                \n",
        "  def get_docs_embedding(self,docs_tok, model, dim=300):\n",
        "           all_docs_embedding = []\n",
        "           for doc in docs_tok:\n",
        "               all_docs_embedding.append(self.text2vec(doc, model, dim))\n",
        "           return np.array(all_docs_embedding)\n",
        "          \n",
        "      \n",
        "    \n",
        "             \n",
        "  def cal_embeding(self):\n",
        "    my_list=[]\n",
        "    my_list.append(article_text)\n",
        "    mysent_dict ={}\n",
        "    mysent_dict['sent'] = my_list     \n",
        "    mydata = pd.DataFrame.from_dict(mysent_dict)\n",
        "    mydata['processed'] = mydata['sent'].map(lambda x: S.process_text(x))\n",
        "    em_model = self.init_model()\n",
        "    my_headlines_embedding = self.get_docs_embedding(mydata['processed'], em_model)\n",
        "    return my_headlines_embedding,mydata\n",
        "\n",
        "\n",
        "  def get_relevant_words(self,search_tok, doc_tok, model):\n",
        "    search_set = set()\n",
        "    doc_set = set()\n",
        "    word_array = set()\n",
        "    for word in search_tok:\n",
        "        if word in model:\n",
        "            search_set.add(word)\n",
        "\n",
        "    for word in doc_tok:\n",
        "        if word in model:\n",
        "            doc_set.add(word)\n",
        "            \n",
        "    for s in  product(search_set, doc_set):\n",
        "        if model.similarity(s[0], s[1]) >= 0.45:\n",
        "            word_array.add(s[1])\n",
        "    return ', '.join(list(word_array))\n",
        "  \n",
        "  \n",
        "  def semantic_search_cossim(self,search_string, docs_embeddings, em_model, data, topn=10):\n",
        "    process_search_str = self.process_text(search_string)\n",
        "    search_string_vect = np.array(self.text2vec(process_search_str, em_model)).reshape(1, -1)\n",
        "    cosine_similarities = pd.Series(cosine_similarity(search_string_vect, docs_embeddings)[0])\n",
        "    result_df = pd.DataFrame(columns=['relevant_words', 'similarity'], index=range(topn))\n",
        "    \n",
        "    k = 0\n",
        "    \n",
        "    for i, j in cosine_similarities.nlargest(int(topn)).iteritems():\n",
        "        result_df['relevant_words'][k] = self.get_relevant_words(search_tok=process_search_str,\n",
        "                                                            doc_tok=data.processed[i],\n",
        "                                                            model=em_model)\n",
        "        result_df['similarity'][k] = j\n",
        "        k += 1\n",
        "\n",
        "    return result_df\n",
        "\n",
        "  def cal_words(self):\n",
        "      search_headline = input(\"Products/Services : \")\n",
        "      my_headlines_embedding,mydata= self.cal_embeding()\n",
        "      em_model = self.init_model()\n",
        "      results = self.semantic_search_cossim(search_headline, my_headlines_embedding, em_model,\n",
        "                                 mydata, topn=1)\n",
        "\n",
        "\n",
        "      print(\"\\nMost important words: \",list(results['relevant_words']))\n",
        "      print(\"\\nCosine Similarities: \",list(results['similarity']))\n",
        "  \n",
        "      \n",
        "      \n",
        "\n",
        "\n",
        "model_path = '/content/drive/MyDrive/Nanos_test/crawl-300d-2M.magnitude'\n",
        "url = str(input(\"Enter Valid URL: \"))\n",
        "S=Similarity(url,model_path)\n",
        "\n",
        "S.cal_words()\n",
        "\n",
        "\n",
        "# https://nanos.ai/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-56b76b6a34fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpymagnitude\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMagnitudeUtils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpymagnitude\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMagnitude\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpymagnitude\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMagnitude\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pymagnitude'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rv7CY168nt4t"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}