{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Nanos_Test_Final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1rhyGrIvVEIHc0I1gCzMN2GZaBQ5qYOsi",
      "authorship_tag": "ABX9TyNU7fnR3omotbSe1NHt9vHW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prathamsss/Relevant_Words_Analysis_NLP/blob/main/Nanos_Test_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8SH6M5GCh4p"
      },
      "source": [
        "import bs4 as bs\n",
        "import urllib.request\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcoOdHRGCtbN",
        "outputId": "399252eb-796f-43ad-ab5d-237f46f3e151"
      },
      "source": [
        "! echo \"Installing Magnitude.... (please wait, can take a while)\"\n",
        "! (curl https://raw.githubusercontent.com/plasticityai/magnitude/master/install-colab.sh | /bin/bash 1>/dev/null 2>/dev/null)\n",
        "! echo \"Done installing Magnitude.\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Installing Magnitude.... (please wait, can take a while)\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   137  100   137    0     0    992      0 --:--:-- --:--:-- --:--:--   985\n",
            "Done installing Magnitude.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzVsSho3D9YD",
        "outputId": "cd1cfde1-b1eb-4c0b-d691-462ee5ce61d6"
      },
      "source": [
        "nltk.download('all')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ja-Q9KgfDtV2",
        "outputId": "045570f8-a327-4cad-fc52-a368a3fe0f58"
      },
      "source": [
        "# !unzip /content/drive/MyDrive/million-headlines.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/MyDrive/million-headlines.zip\n",
            "  inflating: abcnews-date-text.csv   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "id": "fJdRAB2KCkgM",
        "outputId": "d3be2f3c-35b3-499f-96aa-ef98ca9125f9"
      },
      "source": [
        "\n",
        "scrapped_data = urllib.request.urlopen('https://nanos.ai/')\n",
        "article = scrapped_data .read()\n",
        "\n",
        "parsed_article = bs.BeautifulSoup(article,'lxml')\n",
        "\n",
        "paragraphs = parsed_article.find_all('p')\n",
        "\n",
        "article_text = \"\"\n",
        "\n",
        "for p in paragraphs:\n",
        "    article_text += p.text\n",
        "\n",
        "article_text    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Create optimized ad campaigns on Google, Facebook, and Instagram all in one place with Nanos patented Artificial Intelligence technologyNo website, no Facebook Business page requiredOnline marketing should be this simpleIt‚Äôs like having an agency in your pocket!Step 1: Sign up at Nanos and tell us in a few words about your great product or serviceStep 2: Nanos AI will suggest ad text, keywords, interests, and platforms for your adStep 3: Decide on the budget you want to spend, and how long to run your adsStep 4: Log in to your dashboard and watch Nanos AI in action optimizing at lightning speedWhether you have a physical store or an online venture, the goal is the same: to grow your business!We understand that existing marketing tools may be too complicated and expensive. After all, not everyone is a marketing expert or can afford to hire one.\\nNo website or landing page? We‚Äôve got you covered!At Nanos, we take the confusion out of online advertising. Nanos fees are transparent: we charge 17% of your total advertising budget, and you can pause or cancel your ad campaign any time. Any remaining funds get refunded to your account automatically!\\nNanos has a proven track record for bringing in new clients for business like yours: restaurants, auto sales, real estate agencies, video game developers, online courses, and wedding planners.Are you running a small to midsize agency?\\nLooking for a tool to optimize your clients‚Äô budgets and your own costs on managing the overhead expenses for ad campaign design, text, placement and cross-platform, keywords and interests optimization, and reporting?At Nanos, we help make success stories one business at a time.Thanks to Nanocorp AG, I can create my online marketing campaign with a single login and use the effective evaluation to control and adapt the online marketing strategy of my company constantly. Thanks to the great support and experience of the team, I receive useful tips in terms of online marketing and business success.Nanos makes it possible: Finally, we can incorporate online advertising into the communication mix for our customers with small budgets. The results are more than convincing and the numbers are getting better everyday thanks to Nanos. Working with the entire Nanos team is very cooperative and efficient.I struggled a lot with digital advertising, hired lots of freelancers, but I could never find anyone I could really trust that would generate high performances. With Nanos, I can create campaigns for my clients, knowing that Nanos technology will do the best for me ‚Ä¶ And I love that they‚Äôre always available via chat!As a startup, I wanted to have a tool which would allow me to  build my brand Happy Pijama and to grow sales in a simple and clear way, without spending lots of time and people resources. During Web Summit 2019 in Lisbon, I came across Nanos where they presented their tool and I thought, ‚ÄúThis is exactly what I need,‚Äù so I placed my campaigns. Placing online ads on Nanos is a really smooth process. All in all, I had a really great experience for the post-ad period. The reports for each category are pretty detailed.We at Physio & Co are very grateful to be introduced to Nanos. Nanos took the time to understand our business and our needs, and was able to offer simple, affordable solutions. Our patients can now find us easily and thanks to Nanos, we now have an efficient, effective marketing channel, with almost zero effort from our side. Thank you Nanos üôÇFrom a tech perspective, Nanos is an up-and-coming marketing technology innovation that‚Äôs literally saved us time when it comes to placing online ads. The interface is much more pleasant to work with and the ML tech that runs it is unprecedented. I‚Äôve used it for my business and I will use it again.\\nread moreWhat I like about working with Nanos is that there is one interface that allows setting things up on a lot of different platforms. I like the ease of use of the system ‚Äì it‚Äôs not too complex, you can figure it out fairly quickly.Nanos powerful technology helps you craft a campaign with multiple ads and publishes them simultaneously on several platforms:\\xa0Google Ads, Facebook Ads, and Instagram.Our AI suggests the best platforms based on your product. During the campaign run, the machine learning technology reshuffles your budget, discarding the lowest performing platforms.Nanos technology also edits your campaign on the fly by optimizing keywords, bids and interests, all while experimenting with audiences. It‚Äôs like having your own agency, 24/7!Curious about our technology?Sign up for our newsletter to make sure you don‚Äôt miss a thingBecome a partner and ambassador of Nanos to be part of a network of strategic collaborations that give businesses big and small access to marketing power they need!Necessary cookies are absolutely essential for the website to function properly. This category only includes cookies that ensures basic functionalities and security features of the website. These cookies do not store any personal information.Any cookies that may not be particularly necessary for the website to function and is used specifically to collect user personal data via analytics, ads, other embedded contents are termed as non-necessary cookies. It is mandatory to procure user consent prior to running these cookies on your website.About Us\\nFAQ\\nContactVideos\\nNanos AI Engine\\nCase Studies\\nBlog\\nBuild a Free Landing Page\\nBook a Free ConsultationTerms of Use\\nPrivacy PolicyNanos ¬© 2021'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9j_0yBsRC50p"
      },
      "source": [
        "\n",
        "import pandas as pd\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def process_text(text):\n",
        "  text = text.lower()\n",
        "  text=text.replace('{html}',\"\") \n",
        "  cleanr = re.compile('<.*?>')\n",
        "  text = re.sub(cleanr, '', text)\n",
        "  rem_url=re.sub(r'http\\S+', '',text)\n",
        "  text = re.sub('[0-9]+', '', rem_url)\n",
        "  \n",
        "  return [token.text for token in nlp(text) if not token.is_stop]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 76
        },
        "id": "DzwYLYXRCmWv",
        "outputId": "63070c56-6381-4613-f91b-ea35148d141d"
      },
      "source": [
        "# sentences=(nltk.tokenize.sent_tokenize(article_text))\n",
        "# sentences = sentences[:2]\n",
        "my_list=[]\n",
        "my_list.append(article_text)\n",
        "mysent_dict ={}\n",
        "mysent_dict['sent'] = my_list\n",
        "mysent_dict\n",
        "\n",
        "mydata = pd.DataFrame.from_dict(mysent_dict)\n",
        "mydata"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sent</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Create optimized ad campaigns on Google, Faceb...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                sent\n",
              "0  Create optimized ad campaigns on Google, Faceb..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "degug9lOD7Pv",
        "outputId": "aa3c06e8-63f2-4d7b-d770-fb8a361f4c92"
      },
      "source": [
        "mydata['processed'] = mydata['sent'].map(lambda x: process_text(x))\n",
        "(mydata['processed'])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    [create, optimized, ad, campaigns, google, ,, ...\n",
              "Name: processed, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-QVLypjJunn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbNLSrZOE_SB"
      },
      "source": [
        "from pymagnitude import Magnitude\n",
        "\n",
        "def init_model():\n",
        "  model = Magnitude('/content/drive/MyDrive/crawl-300d-2M.magnitude')\n",
        "  return model\n",
        "\n",
        "em_model = init_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAeKDEtdFCH-"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def text2vec(doc_tok, model, dim=300):\n",
        "    doc_embedding = np.zeros(dim)\n",
        "    valid_words = 0\n",
        "    # print((doc_tok))\n",
        "    for word in doc_tok:\n",
        "        \n",
        "        if word in model:\n",
        "            \n",
        "            valid_words += 1\n",
        "            doc_embedding += model.query(word)\n",
        "        else:\n",
        "            continue\n",
        "    if valid_words > 0:\n",
        "        return doc_embedding / valid_words\n",
        "    else:\n",
        "        return doc_embedding\n",
        "\n",
        "def mytext2vec(doc_tok, model, dim=300):\n",
        "    doc_embedding = np.zeros(dim)\n",
        "    valid_words = 0\n",
        "    # print((doc_tok))\n",
        "    for word in doc_tok:\n",
        "        \n",
        "        if word in model:\n",
        "            \n",
        "            valid_words += 1\n",
        "            doc_embedding += model.query(word)\n",
        "        else:\n",
        "            continue\n",
        "    if valid_words > 0:\n",
        "        return doc_embedding / valid_words\n",
        "    else:\n",
        "        return doc_embedding\n",
        "\n",
        "   \n",
        "        \n",
        "def get_docs_embedding(docs_tok, model, dim=300):\n",
        "    all_docs_embedding = []\n",
        "    for doc in docs_tok:\n",
        "        all_docs_embedding.append(text2vec(doc, model, dim))\n",
        "    return np.array(all_docs_embedding)\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQ5WXTCpFFRy"
      },
      "source": [
        "headlines_embedding = get_docs_embedding(docs_tok=data['processed_headlines'], model=em_model)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAM6-sA_FIlm"
      },
      "source": [
        "my_headlines_embedding = get_docs_embedding(mydata['processed'], model=em_model)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYsXF0Y7FXqB"
      },
      "source": [
        "from itertools import product\n",
        "\n",
        "def get_relevant_words(search_tok, doc_tok, model):\n",
        "    search_set = set()\n",
        "    doc_set = set()\n",
        "    word_array = set()\n",
        "    for word in search_tok:\n",
        "        if word in model:\n",
        "            search_set.add(word)\n",
        "\n",
        "    for word in doc_tok:\n",
        "        if word in model:\n",
        "            doc_set.add(word)\n",
        "            \n",
        "    for s in  product(search_set, doc_set):\n",
        "        if model.similarity(s[0], s[1]) >= 0.45:\n",
        "            word_array.add(s[1])\n",
        "    return ', '.join(list(word_array))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nP8_y-pmFzTN",
        "outputId": "30b8bb08-4de5-4dc4-fc36-4523c500cd54"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def semantic_search_cossim(search_string, docs_embeddings, em_model, data, topn=10):\n",
        "    # clean search text\n",
        "    process_search_str = process_text(search_string)\n",
        "    # convert search text to vec\n",
        "    search_string_vect = np.array(text2vec(process_search_str, em_model)).reshape(1, -1)\n",
        "    # find cosine similarity b/w search text and document headlines\n",
        "    cosine_similarities = pd.Series(cosine_similarity(search_string_vect, docs_embeddings)[0])\n",
        "    # create an empty dataframe to write output to\n",
        "    result_df = pd.DataFrame(columns=['relevant_words', 'similarity'], index=range(topn))\n",
        "    k = 0\n",
        "    # write data to output dataframe by sorting in ascending order of cosine similarity\n",
        "    for i, j in cosine_similarities.nlargest(int(topn)).iteritems():\n",
        "        # result_df['headlines_matched'][k] = data['sent'][i]\n",
        "        result_df['relevant_words'][k] = get_relevant_words(search_tok=process_search_str,\n",
        "                                                            doc_tok=data.processed[i],\n",
        "                                                            model=em_model)\n",
        "        result_df['similarity'][k] = j\n",
        "        k += 1\n",
        "\n",
        "    return result_df\n",
        "\n",
        "search_headline = input(\"Search: \")\n",
        "results = semantic_search_cossim(search_headline, my_headlines_embedding, em_model,\n",
        "                                mydata, topn=1)\n",
        "\n",
        "\n",
        "print(\"Your answere = \",list(results['relevant_words']),results['similarity'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Search: digital marketing, digital marketing tool\n",
            "Your answere =  ['useful, tools, product, sales, business, tool, ,, brand, marketing, advertising, digital'] 0    0.684314\n",
            "Name: similarity, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJJ-78gdF5qh",
        "outputId": "8cfa4161-4156-46d1-8731-f4cd4b277bd5"
      },
      "source": [
        "k=(results['relevant_words'])\n",
        "s=(results['similarity'])\n",
        "\n",
        "list(k),list(s)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['useful, tools, product, sales, business, tool, ,, brand, marketing, advertising, digital'],\n",
              " [0.6843143141181023])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naKn3kvjGwoz"
      },
      "source": [
        "https://medium.com/analytics-vidhya/basics-of-using-pre-trained-glove-vectors-in-python-d38905f356db\n",
        "\n",
        "https://statsmaths.github.io/stat289-f18/solutions/tutorial19-gensim.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lQmQKIzOhnT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}